<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="
  Embedding Models
  #


  Overview
  #

There are a large number of embedding models built in different time in history. As described in (Hongliu, 2024), there are 4 era:

Count‚Äëbased (e.g. TF‚ÄëIDF, LSA)
Static dense word embeddings (Word2Vec, GloVe, FastText)
ontextualized embeddings (ELMo, GPT, BERT)
Universal text embeddings ‚Äì capable across varied tasks



  Comparison of the Four Eras of Text Embeddings
  #


  
      
          Feature / Era
          1. Count-Based
          2. Static Word Embeddings
          3. Contextualized Embeddings
          4. Universal Text Embeddings
      
  
  
      
          üî§ Unit of Representation
          Word/Sentence
          Word
          Word
          Sentence/Text
      
      
          üßÆ Vector Type
          Sparse
          Dense (fixed-size)
          Dense (contextual)
          Dense (general-purpose)
      
      
          üìö Context Used
          ‚ùå No
          ‚ö†Ô∏è Local window
          ‚úÖ Full sentence
          ‚úÖ Full input
      
      
          ‚è≥ Word Order Captured
          ‚ùå No
          ‚ùå No
          ‚úÖ Yes
          ‚úÖ Yes
      
      
          üîÑ Polysemy Support
          ‚ùå No
          ‚ùå No (same &ldquo;bank&rdquo;)
          ‚úÖ Yes
          ‚úÖ Yes
      
      
          üìä Dimensionality
          High (thousands)
          Low (100‚Äì300)
          Medium‚ÄìHigh (768‚Äì2048)
          Compact (256‚Äì1024)
      
      
          ‚ö° Inference Speed
          ‚úÖ Fast
          ‚úÖ Fast
          ‚ùå Slower
          ‚úÖ Fast (post-training)
      
      
          üß™ Typical Models
          TF-IDF, LSA, LDA
          Word2Vec, GloVe, FastText
          ELMo, GPT, BERT
          SimCSE, E5, BGE, Gecko
      
      
          üß∞ Best For
          Simple baselines
          Word similarity/analogy
          Fine-tuned NLP tasks
          Search, clustering, general NLP
      
      
          üìâ Weaknesses
          No semantics
          No context, no polysemy
          Heavy computation
          Still limited on logic/negation
      
  


  Evaluation
  #

According to MTEB (Niklas at el, 2023), bmbedding models are evalulated in different tasks. There are 8 task categories and 56 datasets for different task categories.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="https://iamsunnysun.github.io/myblog/docs/generative-ai/agents/embedding-overview/">
  <meta property="og:site_name" content="My Study Notes">
  <meta property="og:title" content="My Study Notes">
  <meta property="og:description" content="Embedding Models # Overview # There are a large number of embedding models built in different time in history. As described in (Hongliu, 2024), there are 4 era:
Count‚Äëbased (e.g. TF‚ÄëIDF, LSA) Static dense word embeddings (Word2Vec, GloVe, FastText) ontextualized embeddings (ELMo, GPT, BERT) Universal text embeddings ‚Äì capable across varied tasks Comparison of the Four Eras of Text Embeddings # Feature / Era 1. Count-Based 2. Static Word Embeddings 3. Contextualized Embeddings 4. Universal Text Embeddings üî§ Unit of Representation Word/Sentence Word Word Sentence/Text üßÆ Vector Type Sparse Dense (fixed-size) Dense (contextual) Dense (general-purpose) üìö Context Used ‚ùå No ‚ö†Ô∏è Local window ‚úÖ Full sentence ‚úÖ Full input ‚è≥ Word Order Captured ‚ùå No ‚ùå No ‚úÖ Yes ‚úÖ Yes üîÑ Polysemy Support ‚ùå No ‚ùå No (same ‚Äúbank‚Äù) ‚úÖ Yes ‚úÖ Yes üìä Dimensionality High (thousands) Low (100‚Äì300) Medium‚ÄìHigh (768‚Äì2048) Compact (256‚Äì1024) ‚ö° Inference Speed ‚úÖ Fast ‚úÖ Fast ‚ùå Slower ‚úÖ Fast (post-training) üß™ Typical Models TF-IDF, LSA, LDA Word2Vec, GloVe, FastText ELMo, GPT, BERT SimCSE, E5, BGE, Gecko üß∞ Best For Simple baselines Word similarity/analogy Fine-tuned NLP tasks Search, clustering, general NLP üìâ Weaknesses No semantics No context, no polysemy Heavy computation Still limited on logic/negation Evaluation # According to MTEB (Niklas at el, 2023), bmbedding models are evalulated in different tasks. There are 8 task categories and 56 datasets for different task categories.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="website">
<title>Embedding Overview | My Study Notes</title>
<link rel="icon" href="/myblog/favicon.png" >
<link rel="manifest" href="/myblog/manifest.json">
<link rel="canonical" href="https://iamsunnysun.github.io/myblog/docs/generative-ai/agents/embedding-overview/">
<link rel="stylesheet" href="/myblog/book.min.d5e59cb0aa8d22a5813c62cd5e25d3d172489a369c61dc3f622ee96ae1ebb457.css" integrity="sha256-1eWcsKqNIqWBPGLNXiXT0XJImjacYdw/Yi7pauHrtFc=" crossorigin="anonymous">
  <script defer src="/myblog/fuse.min.js"></script>
  <script defer src="/myblog/en.search.min.f6942672a53900d692f3f0ab74cb97be4e4f4b2427a3268e098238089c49357a.js" integrity="sha256-9pQmcqU5ANaS8/CrdMuXvk5PSyQnoyaOCYI4CJxJNXo=" crossorigin="anonymous"></script>
<link rel="alternate" type="application/rss+xml" href="https://iamsunnysun.github.io/myblog/docs/generative-ai/agents/embedding-overview/index.xml" title="My Study Notes" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/myblog/"><span>My Study Notes</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>















  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/myblog/docs/deep-learning/" class="">Deep Learning</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/myblog/docs/generative-ai/" class="">Generative AI</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/myblog/docs/generative-ai/agents/article1/" class="">Article1</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/myblog/docs/generative-ai/agents/embedding-overview/" class="active">Embedding Overview</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/myblog/docs/generative-ai/context-engineering-survey/" class="">Context Engineering Survey</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/myblog/docs/generative-ai/context-engineering/" class="">Context Engineering</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/myblog/docs/machine-learning/" class="">Machine Learning</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>










  
<ul>
  
  <li>
    <a href="/myblog/posts/"  >
        Blog
      </a>
  </li>
  
</ul>






</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/myblog/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>Embedding Overview</h3>

  <label for="toc-control">
    
    <img src="/myblog/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#overview">Overview</a></li>
    <li><a href="#comparison-of-the-four-eras-of-text-embeddings">Comparison of the Four Eras of Text Embeddings</a></li>
    <li><a href="#evaluation">Evaluation</a>
      <ul>
        <li><a href="#finding">Finding</a></li>
      </ul>
    </li>
    <li><a href="#domain-specific-embedding-model">Domain specific embedding model</a>
      <ul>
        <li><a href="#why-we-need-a-domain-specific-model">Why we need a domain specific model</a></li>
        <li><a href="#quantified-result">Quantified result</a></li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="embedding-models">
  Embedding Models
  <a class="anchor" href="#embedding-models">#</a>
</h1>
<h2 id="overview">
  Overview
  <a class="anchor" href="#overview">#</a>
</h2>
<p>There are a large number of embedding models built in different time in history. As described in (<a href="https://arxiv.org/pdf/2406.01607v1">Hongliu, 2024</a>), there are 4 era:</p>
<ul>
<li>Count‚Äëbased (e.g. TF‚ÄëIDF, LSA)</li>
<li>Static dense word embeddings (Word2Vec, GloVe, FastText)</li>
<li>ontextualized embeddings (ELMo, GPT, BERT)</li>
<li>Universal text embeddings ‚Äì capable across varied tasks</li>
</ul>
<p><img src="image.png" alt="alt text" /></p>
<h2 id="comparison-of-the-four-eras-of-text-embeddings">
  Comparison of the Four Eras of Text Embeddings
  <a class="anchor" href="#comparison-of-the-four-eras-of-text-embeddings">#</a>
</h2>
<table>
  <thead>
      <tr>
          <th>Feature / Era</th>
          <th>1. Count-Based</th>
          <th>2. Static Word Embeddings</th>
          <th>3. Contextualized Embeddings</th>
          <th>4. Universal Text Embeddings</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>üî§ Unit of Representation</td>
          <td>Word/Sentence</td>
          <td>Word</td>
          <td>Word</td>
          <td>Sentence/Text</td>
      </tr>
      <tr>
          <td>üßÆ Vector Type</td>
          <td>Sparse</td>
          <td>Dense (fixed-size)</td>
          <td>Dense (contextual)</td>
          <td>Dense (general-purpose)</td>
      </tr>
      <tr>
          <td>üìö Context Used</td>
          <td>‚ùå No</td>
          <td>‚ö†Ô∏è Local window</td>
          <td>‚úÖ Full sentence</td>
          <td>‚úÖ Full input</td>
      </tr>
      <tr>
          <td>‚è≥ Word Order Captured</td>
          <td>‚ùå No</td>
          <td>‚ùå No</td>
          <td>‚úÖ Yes</td>
          <td>‚úÖ Yes</td>
      </tr>
      <tr>
          <td>üîÑ Polysemy Support</td>
          <td>‚ùå No</td>
          <td>‚ùå No (same &ldquo;bank&rdquo;)</td>
          <td>‚úÖ Yes</td>
          <td>‚úÖ Yes</td>
      </tr>
      <tr>
          <td>üìä Dimensionality</td>
          <td>High (thousands)</td>
          <td>Low (100‚Äì300)</td>
          <td>Medium‚ÄìHigh (768‚Äì2048)</td>
          <td>Compact (256‚Äì1024)</td>
      </tr>
      <tr>
          <td>‚ö° Inference Speed</td>
          <td>‚úÖ Fast</td>
          <td>‚úÖ Fast</td>
          <td>‚ùå Slower</td>
          <td>‚úÖ Fast (post-training)</td>
      </tr>
      <tr>
          <td>üß™ Typical Models</td>
          <td>TF-IDF, LSA, LDA</td>
          <td>Word2Vec, GloVe, FastText</td>
          <td>ELMo, GPT, BERT</td>
          <td>SimCSE, E5, BGE, Gecko</td>
      </tr>
      <tr>
          <td>üß∞ Best For</td>
          <td>Simple baselines</td>
          <td>Word similarity/analogy</td>
          <td>Fine-tuned NLP tasks</td>
          <td>Search, clustering, general NLP</td>
      </tr>
      <tr>
          <td>üìâ Weaknesses</td>
          <td>No semantics</td>
          <td>No context, no polysemy</td>
          <td>Heavy computation</td>
          <td>Still limited on logic/negation</td>
      </tr>
  </tbody>
</table>
<h2 id="evaluation">
  Evaluation
  <a class="anchor" href="#evaluation">#</a>
</h2>
<p>According to MTEB (<a href="https://arxiv.org/pdf/2210.07316">Niklas at el, 2023</a>), bmbedding models are evalulated in different tasks. There are 8 task categories and 56 datasets for different task categories.</p>
<p><img src="image-1.png" alt="alt text" /></p>
<table>
  <thead>
      <tr>
          <th>Task Type</th>
          <th>What It Measures</th>
          <th>Example Dataset</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Retrieval</strong></td>
          <td>How well embeddings retrieve relevant docs</td>
          <td>MS MARCO, BEIR</td>
      </tr>
      <tr>
          <td><strong>Reranking</strong></td>
          <td>Rank candidates by relevance</td>
          <td>TREC-COVID, SciFact</td>
      </tr>
      <tr>
          <td><strong>Clustering</strong></td>
          <td>Group similar texts</td>
          <td>Arxiv, StackExchange</td>
      </tr>
      <tr>
          <td><strong>Classification</strong></td>
          <td>Use embeddings for supervised tasks</td>
          <td>Amazon Reviews</td>
      </tr>
      <tr>
          <td><strong>STS (Similarity)</strong></td>
          <td>How similar two texts are</td>
          <td>STSBenchmark, SICK</td>
      </tr>
      <tr>
          <td><strong>Pair Classification</strong></td>
          <td>Textual entailment, semantic match</td>
          <td>MRPC, PAWS</td>
      </tr>
      <tr>
          <td><strong>Summarization</strong></td>
          <td>Embedding-level summarization quality</td>
          <td>XSum, Reddit TLDR</td>
      </tr>
      <tr>
          <td><strong>Question Answering</strong></td>
          <td>Open-domain QA via embedding search</td>
          <td>NQ, HotpotQA</td>
      </tr>
  </tbody>
</table>
<h3 id="finding">
  Finding
  <a class="anchor" href="#finding">#</a>
</h3>
<ul>
<li>Universal embedding models (e.g., E5, BGE, Gecko) generalize well across multiple MTEB tasks.</li>
<li>Models like SimCSE are strong baselines, but newer approaches significantly outperform them, especially on retrieval and clustering.
<ul>
<li>Retrieval +100% or more (massive gains)</li>
<li>Reranking +22‚Äì28%</li>
<li>Clustering +35‚Äì57%</li>
<li>Pair Classification +15‚Äì20%</li>
<li>Gains on Semantic Textual Similarity (STS) more modest (~8%)</li>
<li>Summarization tasks saw no real improvement over baseline</li>
</ul>
</li>
<li>However, summarization and negation sensitivity remain weak spots.
<ul>
<li>Negation sensitivity: current embeddings poorly distinguish negated sentences (&ldquo;happy&rdquo; vs &ldquo;not happy&rdquo;). That issue is picked up in follow‚Äëon work showing sub‚Äë1% improvements in negation-aware benchmarks unless special training or reweighting is applied‚ÄØ</li>
<li>Multilingual / domain coverage: most models are English‚Äëcentric and evaluated on similar domains to training data (e.g. QA, Reddit)‚Äîthus generalization across diverse real‚Äëworld domains (finance, health, culture) is underexplored‚ÄØ</li>
<li>Summarization task gap: no top embedding beats SimCSE baseline</li>
</ul>
</li>
</ul>
<h2 id="domain-specific-embedding-model">
  Domain specific embedding model
  <a class="anchor" href="#domain-specific-embedding-model">#</a>
</h2>
<p>In this paper (<a href="https://arxiv.org/pdf/2409.18511v3">Yixuan et al, 2024</a>), author analysis why we need a domain specific model and proposed some dataset and benchmark for Finance.</p>
<h3 id="why-we-need-a-domain-specific-model">
  Why we need a domain specific model
  <a class="anchor" href="#why-we-need-a-domain-specific-model">#</a>
</h3>
<p>In general there are following resons:</p>
<ol>
<li>
<p>Specialized Vocabulary</p>
<p>General models are trained on web-scale corpora (Wikipedia, Reddit, etc.). Domains like finance use terms like &ldquo;yield curve inversion,&rdquo; &ldquo;credit default swap,&rdquo; &ldquo;EBITDA&rdquo; ‚Äî which general models rarely see or misinterpret.</p>
<p>Example:
A general model might equate ‚Äúliquidity‚Äù with ‚Äúfluid‚Äù rather than ‚Äúavailable capital.‚Äù</p>
</li>
<li>
<p>Different Semantics</p>
<p>Words can mean different things depending on the context:</p>
<ul>
<li>‚ÄúMargin‚Äù in finance (loan collateral)</li>
<li>‚ÄúMargin‚Äù in design (white space)</li>
<li>‚ÄúOperation‚Äù in math, medicine, or military</li>
</ul>
<p>Domain models learn the correct sense in context.</p>
</li>
<li>
<p>Formal/Technical Language</p>
<ul>
<li>Sentences are often long, formal, and complex.</li>
<li>Domain documents (e.g., financial reports, legal filings, scientific papers) include jargon and unusual syntax.</li>
<li>General models are not trained to handle this effectively.</li>
</ul>
<p>The paper showed that ChatGPT&rsquo;s own error rate is higher on financial tasks compared to general ones ‚Äî meaning even top-tier LLMs struggle.</p>
</li>
<li>
<p>Domain-Specific Reasoning</p>
<ul>
<li>In finance, conclusions often depend on economic logic or quantitative reasoning (e.g., interpreting balance sheets).</li>
<li>In medicine, models must connect symptoms, diagnoses, and treatments ‚Äî general embeddings can&rsquo;t model this well without extra training.</li>
</ul>
</li>
<li>
<p>Empirical Evidence (from FinMTEB)</p>
<ul>
<li>General-purpose embeddings perform up to 100% worse on finance tasks vs general ones.</li>
<li>There&rsquo;s no reliable correlation between model performance on MTEB (general) and FinMTEB (domain-specific).</li>
<li>So, even top-ranked general models can&rsquo;t be trusted in domain applications without re-evaluation or adaptation.</li>
</ul>
</li>
</ol>
<h3 id="quantified-result">
  Quantified result
  <a class="anchor" href="#quantified-result">#</a>
</h3>
<p>There are some quantified result on how the general embedding model vs. domain specific embedding model.</p>
<p>Four different index are proposed to measure data complexity.</p>
<ul>
<li><strong>ChatGPT Error Rate</strong>. The first measure quantifies how challenging it is for ChatGPT to answer a dataset‚Äôs questions.</li>
<li><strong>Information Theory</strong>. We borrow the concept of information entropy from information theory to measure the complexity of a text sequence.</li>
<li><strong>Readability</strong>. We also use readability to measure dataset complexity, specifically applying the Gunning Fog Index (Gunning, 1952), which factors in sentence length and the number of complex words</li>
<li><strong>Mean Dependency Distance</strong>. Finally, we measure linguistic complexity using the dependency distance between two syntactically related words in a sentence (Oya, 2011). A longer dependency distance indicates that more context is needed for comprehension, reflecting greater sentence complexity.</li>
</ul>
<p>A subgroup analysis is conducted to examine the impact of the domain on embedding model per-
formance.</p>
<ul>
<li>First, dataset complexity is calculated using one of the four complexity measures and categorize the datasets into three subgroups: low, medium, and high complexity. This ensures that METB and FinMTEB datasets within each subgroup have the same level of complexity.</li>
<li>Then, the average performance score of seven LLM-based embedding models across datasets is calculated within each group.</li>
</ul>
<p>The result is shown as below:
<img src="image-2.png" alt="alt text" />
It can be observed that:</p>
<ul>
<li>First, embedding models perform substantially worse on FinMTEB datasets compared to MTEB datasets, even after accounting for dataset complexity.</li>
<li>Second, embedding models perform worst on FinMTEB datasets with the highest complexity levels.</li>
</ul>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>





  
  
  
  <div class="flex flex-wrap justify-between">
    <span>
    
      <a href="/myblog/docs/generative-ai/agents/article1/" class="flex align-center book-icon">
        <img src="/myblog/svg/backward.svg" class="book-icon" alt="Previous" title="Article1" />
        <span>Article1</span>
      </a>
    
    </span>
    <span>
    
      <a href="/myblog/docs/generative-ai/context-engineering-survey/" class="flex align-center book-icon">
        <span>Context Engineering Survey</span>
        <img src="/myblog/svg/forward.svg" class="book-icon" alt="Next" title="Context Engineering Survey" />
      </a>
    
    </span>
  </div>
  




  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#overview">Overview</a></li>
    <li><a href="#comparison-of-the-four-eras-of-text-embeddings">Comparison of the Four Eras of Text Embeddings</a></li>
    <li><a href="#evaluation">Evaluation</a>
      <ul>
        <li><a href="#finding">Finding</a></li>
      </ul>
    </li>
    <li><a href="#domain-specific-embedding-model">Domain specific embedding model</a>
      <ul>
        <li><a href="#why-we-need-a-domain-specific-model">Why we need a domain specific model</a></li>
        <li><a href="#quantified-result">Quantified result</a></li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












