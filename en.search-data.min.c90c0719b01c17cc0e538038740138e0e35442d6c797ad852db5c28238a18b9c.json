[{"id":0,"href":"/myblog/docs/deep-learning/","title":"Deep Learning","section":"Docs","content":" Deep learning # Information about deep neuro network.\n"},{"id":1,"href":"/myblog/docs/generative-ai/","title":"Generative AI","section":"Docs","content":" Generative AI # Information about generative AI\n"},{"id":2,"href":"/myblog/docs/machine-learning/","title":"Machine Learning","section":"Docs","content":" Machine Learning # Some basic knowledge of machine learning\n"},{"id":3,"href":"/myblog/posts/my-second-post/","title":"My Second Post","section":"Blog","content":" Introduction # This is bold text, and this is emphasized text.\nVisit the Hugo website!\n"},{"id":4,"href":"/myblog/posts/my-first-post/","title":"My First Post","section":"Blog","content":"this is a test\n"},{"id":5,"href":"/myblog/docs/generative-ai/agents/article1/","title":"Article1","section":"Generative AI","content":" Document 1 # this document is an example. # here is some inline images My Test Image\nhere is another image in native markdown format My Math Post # \\[ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi \\] Euler\u0026rsquo;s identity is famous: $ e^{i\\pi} + 1 = 0 $. Euler\u0026rsquo;s identity is ( e^{i\\pi} + 1 = 0 )\nEuler\u0026rsquo;s identity is (e^{i\\pi} + 1 = 0) Euler\u0026rsquo;s identity is ( e^{i\\pi} + 1 = 0 )\nAnd Einstein\u0026rsquo;s formula:\n$$ E = mc^2 $$\nThis is another example $$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi),e^{2 \\pi i \\xi x},d\\xi $$\ntest in UML # here is some UML\n@startuml Alice -\u0026gt; Bob: Hello Bob --\u0026gt; Alice: Hi! @enduml here it is\n@startuml actor User User -\u0026gt; System : Login System -\u0026gt; Database : Check credentials Database --\u0026gt; System : Return result System --\u0026gt; User : Success or failure @enduml test diagram # here is some diagram\nstateDiagram-v2 State1: The state with a note note right of State1 Important information! You can write notes. end note State1 --\u003e State2 note left of State2 : This is the note to the left. "},{"id":6,"href":"/myblog/docs/generative-ai/agents/embedding-overview/","title":"Embedding Overview","section":"Generative AI","content":" Embedding Models # Overview # There are a large number of embedding models built in different time in history. As described in (Hongliu, 2024), there are 4 era:\nCount‚Äëbased (e.g. TF‚ÄëIDF, LSA) Static dense word embeddings (Word2Vec, GloVe, FastText) ontextualized embeddings (ELMo, GPT, BERT) Universal text embeddings ‚Äì capable across varied tasks Comparison of the Four Eras of Text Embeddings # Feature / Era 1. Count-Based 2. Static Word Embeddings 3. Contextualized Embeddings 4. Universal Text Embeddings üî§ Unit of Representation Word/Sentence Word Word Sentence/Text üßÆ Vector Type Sparse Dense (fixed-size) Dense (contextual) Dense (general-purpose) üìö Context Used ‚ùå No ‚ö†Ô∏è Local window ‚úÖ Full sentence ‚úÖ Full input ‚è≥ Word Order Captured ‚ùå No ‚ùå No ‚úÖ Yes ‚úÖ Yes üîÑ Polysemy Support ‚ùå No ‚ùå No (same \u0026ldquo;bank\u0026rdquo;) ‚úÖ Yes ‚úÖ Yes üìä Dimensionality High (thousands) Low (100‚Äì300) Medium‚ÄìHigh (768‚Äì2048) Compact (256‚Äì1024) ‚ö° Inference Speed ‚úÖ Fast ‚úÖ Fast ‚ùå Slower ‚úÖ Fast (post-training) üß™ Typical Models TF-IDF, LSA, LDA Word2Vec, GloVe, FastText ELMo, GPT, BERT SimCSE, E5, BGE, Gecko üß∞ Best For Simple baselines Word similarity/analogy Fine-tuned NLP tasks Search, clustering, general NLP üìâ Weaknesses No semantics No context, no polysemy Heavy computation Still limited on logic/negation Evaluation # According to MTEB (Niklas at el, 2023), bmbedding models are evalulated in different tasks. There are 8 task categories and 56 datasets for different task categories.\nTask Type What It Measures Example Dataset Retrieval How well embeddings retrieve relevant docs MS MARCO, BEIR Reranking Rank candidates by relevance TREC-COVID, SciFact Clustering Group similar texts Arxiv, StackExchange Classification Use embeddings for supervised tasks Amazon Reviews STS (Similarity) How similar two texts are STSBenchmark, SICK Pair Classification Textual entailment, semantic match MRPC, PAWS Summarization Embedding-level summarization quality XSum, Reddit TLDR Question Answering Open-domain QA via embedding search NQ, HotpotQA Finding # Universal embedding models (e.g., E5, BGE, Gecko) generalize well across multiple MTEB tasks. Models like SimCSE are strong baselines, but newer approaches significantly outperform them, especially on retrieval and clustering. Retrieval +100% or more (massive gains) Reranking +22‚Äì28% Clustering +35‚Äì57% Pair Classification +15‚Äì20% Gains on Semantic Textual Similarity (STS) more modest (~8%) Summarization tasks saw no real improvement over baseline However, summarization and negation sensitivity remain weak spots. Negation sensitivity: current embeddings poorly distinguish negated sentences (\u0026ldquo;happy\u0026rdquo; vs \u0026ldquo;not happy\u0026rdquo;). That issue is picked up in follow‚Äëon work showing sub‚Äë1% improvements in negation-aware benchmarks unless special training or reweighting is applied‚ÄØMultilingual / domain coverage: most models are English‚Äëcentric and evaluated on similar domains to training data (e.g. QA, Reddit)‚Äîthus generalization across diverse real‚Äëworld domains (finance, health, culture) is underexplored‚ÄØSummarization task gap: no top embedding beats SimCSE baseline Domain specific embedding model # In this paper (Yixuan et al, 2024), author analysis why we need a domain specific model and proposed some dataset and benchmark for Finance.\nWhy we need a domain specific model # In general there are following resons:\nSpecialized Vocabulary\nGeneral models are trained on web-scale corpora (Wikipedia, Reddit, etc.). Domains like finance use terms like \u0026ldquo;yield curve inversion,\u0026rdquo; \u0026ldquo;credit default swap,\u0026rdquo; \u0026ldquo;EBITDA\u0026rdquo; ‚Äî which general models rarely see or misinterpret.\nExample: A general model might equate ‚Äúliquidity‚Äù with ‚Äúfluid‚Äù rather than ‚Äúavailable capital.‚Äù\nDifferent Semantics\nWords can mean different things depending on the context:\n‚ÄúMargin‚Äù in finance (loan collateral) ‚ÄúMargin‚Äù in design (white space) ‚ÄúOperation‚Äù in math, medicine, or military Domain models learn the correct sense in context.\nFormal/Technical Language\nSentences are often long, formal, and complex. Domain documents (e.g., financial reports, legal filings, scientific papers) include jargon and unusual syntax. General models are not trained to handle this effectively. The paper showed that ChatGPT\u0026rsquo;s own error rate is higher on financial tasks compared to general ones ‚Äî meaning even top-tier LLMs struggle.\nDomain-Specific Reasoning\nIn finance, conclusions often depend on economic logic or quantitative reasoning (e.g., interpreting balance sheets). In medicine, models must connect symptoms, diagnoses, and treatments ‚Äî general embeddings can\u0026rsquo;t model this well without extra training. Empirical Evidence (from FinMTEB)\nGeneral-purpose embeddings perform up to 100% worse on finance tasks vs general ones. There\u0026rsquo;s no reliable correlation between model performance on MTEB (general) and FinMTEB (domain-specific). So, even top-ranked general models can\u0026rsquo;t be trusted in domain applications without re-evaluation or adaptation. Quantified result # There are some quantified result on how the general embedding model vs. domain specific embedding model.\nFour different index are proposed to measure data complexity.\nChatGPT Error Rate. The first measure quantifies how challenging it is for ChatGPT to answer a dataset‚Äôs questions. Information Theory. We borrow the concept of information entropy from information theory to measure the complexity of a text sequence. Readability. We also use readability to measure dataset complexity, specifically applying the Gunning Fog Index (Gunning, 1952), which factors in sentence length and the number of complex words Mean Dependency Distance. Finally, we measure linguistic complexity using the dependency distance between two syntactically related words in a sentence (Oya, 2011). A longer dependency distance indicates that more context is needed for comprehension, reflecting greater sentence complexity. A subgroup analysis is conducted to examine the impact of the domain on embedding model per- formance.\nFirst, dataset complexity is calculated using one of the four complexity measures and categorize the datasets into three subgroups: low, medium, and high complexity. This ensures that METB and FinMTEB datasets within each subgroup have the same level of complexity. Then, the average performance score of seven LLM-based embedding models across datasets is calculated within each group. The result is shown as below: It can be observed that:\nFirst, embedding models perform substantially worse on FinMTEB datasets compared to MTEB datasets, even after accounting for dataset complexity. Second, embedding models perform worst on FinMTEB datasets with the highest complexity levels. "},{"id":7,"href":"/myblog/docs/generative-ai/context-engineering-survey/","title":"Context Engineering Survey","section":"Generative AI","content":" Context Engineering Survey # This document is based on this paper (https://arxiv.org/pdf/2507.13334)\nTaxonomy of Context Engineering # Context Engineering Evolution Timeline: Conext includes the following components:\n$C_{instr}$: System instructions and rules (Context Retrieval and Generation). $C_{know}$: External knowledge, retrieved via functions like RAG or from integrated knowledge graphs (RAG; Context Processing). $C_{tools}$: Definitions and signatures of available external tools (Function Calling \u0026amp; Tool-Integrated Reasoning). $C_{mem}$: Persistent information from prior interactions (Memory Systems; Context Management). $C_{state}$: The dynamic state of the user, world, or multi-agent system (Multi-Agent Systems \u0026amp; Orchestration). $C_{query}$: The user‚Äôs immediate request Reference\nhttps://github.com/Meirtz/Awesome-Context-Engineeringc "},{"id":8,"href":"/myblog/docs/generative-ai/context-engineering/","title":"Context Engineering","section":"Generative AI","content":" Context Engineering # Overview # Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\nThere are a few methods to do context engineering Tool loadout # The general idea of Tool loadout is when there are hundreds or thousands of tools, The accuracy will decrease with number tools increased. It is preferred to load tools relevant to user\u0026rsquo;s question.\nAs described in this paper (https://arxiv.org/pdf/2410.14594), the flow is shown below: Pre-retrival\nDescribe tools, include names, descriptions, argument, hypothetical question, topics and metadata. Build these info in vector DB. Intra-retrival\nUse user input to query in vector DB. Get the most relevant tools. Bind the tools in LLM and send the user\u0026rsquo;s query to the LLM with binded tool. Post-retrival\nuse LLM to re-rank the results or do self-RAG to generate final result "}]